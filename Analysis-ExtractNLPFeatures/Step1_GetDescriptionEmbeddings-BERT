{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Step1_GetDescriptionEmbeddings-BERT","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMzVtQF8pnajIdBWDuYo2F+"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"cTSvkCSLAYfW","colab_type":"text"},"source":["# Step 1 in analyzing the Describe data\n","1. Format the Data by putting the descriptions (stored in JSONs) into dictionaries for each subject\n","2. Pass the descriptions through a BERT model (https://github.com/google-research/bert)\n","\n","\n","written by Leyla Tarhan (ltarhan@g.harvard.edu)\n","\n","4/2020"]},{"cell_type":"markdown","metadata":{"id":"wNLHsWTOBZm7","colab_type":"text"},"source":["# Load Dependencies\n","\n"]},{"cell_type":"code","metadata":{"id":"Hk3dtpp0Bi1N","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593556314905,"user_tz":240,"elapsed":3641,"user":{"displayName":"Leyla Tarhan","photoUrl":"","userId":"10856866060736178213"}}},"source":["## load dependencies\n","\n","# !pip install --quiet tensorflow==2.0.0\n","# !pip install --quiet tensorflow-hub\n","# !pip install --quiet seaborn\n","!pip install --quiet sentence_transformers\n","\n","import tensorflow as tf\n","tf.executing_eagerly()\n","import tensorflow_hub as hub\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import pickle\n","import math\n","import csv\n","import pandas as pd\n","from pdb import set_trace\n","import os # making file paths and directories\n","from os import listdir\n","from os.path import isfile, join\n","import json # reading in JSON data\n","import re # regular expressions\n","\n","import torch\n","from sentence_transformers import SentenceTransformer # BERT\n"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uZJkksVaCZNX","colab_type":"text"},"source":["## Connect to data files saved on my google drive"]},{"cell_type":"code","metadata":{"id":"mbwCCkyuCahu","colab_type":"code","colab":{}},"source":["## Connect to data files saved to my google drive\n","\n","from google.colab import drive\n","from google.colab import files\n","drive.mount('/content/gdrive')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EiqM9D4aCxfy","colab_type":"text"},"source":["## Import the data"]},{"cell_type":"code","metadata":{"id":"rsyMI5TXCzKy","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593556320226,"user_tz":240,"elapsed":602,"user":{"displayName":"Leyla Tarhan","photoUrl":"","userId":"10856866060736178213"}}},"source":["# set up file structure:\n","topDir = '/content/gdrive/My Drive/ActionSim'\n","dataDir = 'Data-JSONs'\n","dataPath = os.path.join(topDir, dataDir)\n","# listdir(dataPath) # check out all the files\n","\n","# set up the video sets:\n","sets = ['Set1', 'Set2']\n","# sets = ['Set2']\n","\n","# load in the video structs for each video set (to control the order of the data across subs)\n","itemLabels = {};\n","vidPaths = {};\n","for vs in sets:\n","  # load in the csv as a data frame\n","  df = pd.read_csv(os.path.join(topDir, 'VidStruct-' + vs + '.csv'))\n","  itemLabels[vs] = df.action.values; # load in the action/video names\n","  vidPaths[vs] = df.filepath.values;\n"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"GvasW1lSC7br","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":612},"executionInfo":{"status":"ok","timestamp":1593556323513,"user_tz":240,"elapsed":1530,"user":{"displayName":"Leyla Tarhan","photoUrl":"","userId":"10856866060736178213"}},"outputId":"0eb5e778-4b22-4377-cd12-6afc6286ffd5"},"source":["# loop through the files and load in the descriptions:\n","subCounter = 1\n","allSubsDescriptions = {} # set up a top-level dictionary to store data for all subs\n","for f in listdir(dataPath):\n","  # assign a sub id (in place of the SONA id)\n","  subID = \"Sub\" + str(subCounter)\n","\n","  # initialize a dictionary for this sub:\n","  currSubDict = {}\n","\n","  # read in their JSON file:\n","  with open(os.path.join(dataPath, f)) as i:\n","    json_data = json.load(i) # convert to dictionary\n","    \n","    vidNames = json_data['urls'] # video names: practice & main experiment\n","    descriptions = json_data['descriptions'] # will need to dig deeper into these to get the useful parts\n","    assert len(vidNames) == len(descriptions), 'video names and descriptions are different sizes'\n","      \n","  # figure out which set this is:\n","  currSet = []\n","  for vs in sets:\n","    if vs in vidNames[-1]:\n","      print('current set is ' + vs + '!')\n","      currSet = vs\n","      currSubDict['video set'] = currSet\n","\n","  # grab the relevant list of vid names and paths from the vid struct:\n","  currVidNames = itemLabels[currSet] # same order for all subs\n","  currVidPaths = vidPaths[currSet]\n","\n","  # loop through the vids (in order specified by vid struct):\n","  currSubDescriptions = {}\n","  for v in currVidPaths:\n","    # find the entry that corresponds to this video:\n","    currVidIdx = vidNames.index(v)\n","\n","    # grab this vid's description\n","    descriptionStruct = descriptions[currVidIdx]\n","    currVidDescription = descriptionStruct['transcript']['edited']\n","\n","    # save it to a growing dictionary\n","    currSubDescriptions[v] = currVidDescription\n","\n","  # save everything for this sub (in an uber-dictionary)\n","  currSubDict['descriptions'] = currSubDescriptions\n","  currSubDict['itemLabels'] = currVidNames\n","  allSubsDescriptions[subID] = currSubDict\n","\n","\n","  # iterate the sub id counter:\n","  subCounter = subCounter+1"],"execution_count":21,"outputs":[{"output_type":"stream","text":["current set is Set1!\n","current set is Set2!\n","current set is Set2!\n","current set is Set1!\n","current set is Set1!\n","current set is Set2!\n","current set is Set2!\n","current set is Set1!\n","current set is Set2!\n","current set is Set1!\n","current set is Set1!\n","current set is Set1!\n","current set is Set2!\n","current set is Set1!\n","current set is Set1!\n","current set is Set2!\n","current set is Set2!\n","current set is Set2!\n","current set is Set1!\n","current set is Set1!\n","current set is Set2!\n","current set is Set2!\n","current set is Set1!\n","current set is Set1!\n","current set is Set1!\n","current set is Set1!\n","current set is Set2!\n","current set is Set2!\n","current set is Set1!\n","current set is Set2!\n","current set is Set2!\n","current set is Set2!\n","current set is Set1!\n","current set is Set2!\n","current set is Set2!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vMF_9QHtDsd5","colab_type":"text"},"source":["# Pass the descriptions through the BERT model"]},{"cell_type":"code","metadata":{"id":"gJXqpETrB_W3","colab_type":"code","colab":{}},"source":["## set up the BERT model\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","model_name = 'bert-large-nli-stsb-mean-tokens'\n","model = SentenceTransformer(model_name, device=device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TQcFflYoDptt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":612},"executionInfo":{"status":"ok","timestamp":1593557973906,"user_tz":240,"elapsed":539023,"user":{"displayName":"Leyla Tarhan","photoUrl":"","userId":"10856866060736178213"}},"outputId":"c16e30a2-c5c4-402b-c04c-e4b82643a010"},"source":["# loop through all subs\n","for sub in allSubsDescriptions.keys():\n","  print('getting embeddings for ' + sub + '...')\n","  currSubSet = allSubsDescriptions[sub]['video set']\n","  currSubVidPaths = vidPaths[currSubSet]\n","\n","  # pull out all the descriptions:\n","  descriptionsBare = []\n","  for v in currSubVidPaths:\n","    vidDescr = allSubsDescriptions[sub]['descriptions'][v]\n","    descriptionsBare.append(vidDescr)\n","  # print(descriptionsBare)\n","\n","  # get embeddings for all vids for this subject:\n","  model.eval()\n","  with torch.no_grad():\n","    # embed a list of sentences\n","    currSubEmbeddings = model.encode(descriptionsBare)\n","  # # output: list of numpy arrays -- 1 array per video, with 1024 features\n","  # print(len(currSubEmbeddings)) # 1 array per vid\n","  # print(len(currSubEmbeddings[0]D)) # 1024 features per vid\n","  # print(type(currSubEmbeddings[0]))\n","\n","  # old code form universal sentence encoder model:\n","  # currSubEmbeddings = embed([desc for desc in descriptionsBare])['outputs'].numpy()\n","  # print(currSubEmbeddings.shape) # vids x 512 features\n","\n","  # save the embeddings for this subject (to google drive):\n","  fn = sub + '-' + currSubSet + '-BERTembeddings.csv'\n","  savePath = os.path.join(topDir, 'Data-Embeddings-BERT')\n","  if not(os.path.isdir(savePath)):\n","      os.mkdir(savePath)\n","  fp = os.path.join(savePath, fn)\n","\n","  embeddings_export = pd.DataFrame(currSubEmbeddings) # convert to data frame\n","  # add in row names (for each vid) -- use the action name, not the vid path\n","  embeddings_export.index = allSubsDescriptions[sub]['itemLabels']\n","  embeddings_export.to_csv(fp)\n","  # each row = 1 vid, each col = 1 embedding feature\n","\n"],"execution_count":23,"outputs":[{"output_type":"stream","text":["getting embeddings for Sub1...\n","getting embeddings for Sub2...\n","getting embeddings for Sub3...\n","getting embeddings for Sub4...\n","getting embeddings for Sub5...\n","getting embeddings for Sub6...\n","getting embeddings for Sub7...\n","getting embeddings for Sub8...\n","getting embeddings for Sub9...\n","getting embeddings for Sub10...\n","getting embeddings for Sub11...\n","getting embeddings for Sub12...\n","getting embeddings for Sub13...\n","getting embeddings for Sub14...\n","getting embeddings for Sub15...\n","getting embeddings for Sub16...\n","getting embeddings for Sub17...\n","getting embeddings for Sub18...\n","getting embeddings for Sub19...\n","getting embeddings for Sub20...\n","getting embeddings for Sub21...\n","getting embeddings for Sub22...\n","getting embeddings for Sub23...\n","getting embeddings for Sub24...\n","getting embeddings for Sub25...\n","getting embeddings for Sub26...\n","getting embeddings for Sub27...\n","getting embeddings for Sub28...\n","getting embeddings for Sub29...\n","getting embeddings for Sub30...\n","getting embeddings for Sub31...\n","getting embeddings for Sub32...\n","getting embeddings for Sub33...\n","getting embeddings for Sub34...\n","getting embeddings for Sub35...\n"],"name":"stdout"}]}]}